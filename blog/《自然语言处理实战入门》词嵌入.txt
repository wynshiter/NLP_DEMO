







文章大纲

word2vector



 
word2vector 使用
参考文献




word2vector
Word2vec Word2vec是2013年Google发布的无监督词向embedding模型。该模型采用CBOW或Skip-gram模型来训练词向量，将词从one-hot编码的向量映射成d维稠密向量：
word2vector顾名思义，其实就是旨在把每个单词转化为词向量，其实很多方式都可以实现这个功能，最简单的当然就是one-hot了，但是面对无敌庞大的词库，直接使用one-hot来进行表示将会面临很大的内存占用和很高的计算时间，于是有了LDA、GloVe以及现在比较新的bert等，都是尝试通过使用连续的词向量模型来进行词向量转化，从而进行后续的自然语言处理任务。
那么word2vector是通过什么方式来产生词向量呢——答案就是上下文+网络的模式，而这种上下文如何体现，就是通过类似完形填空的方式进行，对于上下文若干个单词，来预测我挖空位置的单词，这就是CBOW的目前最为常见的方式，看好，是最为常见的方式，即multi-word context（多词语境）形式，这种形式是目前很多文章里面提到的形式，但他的形式并不唯一，这个在《word2vec Parameter Learning Exp




