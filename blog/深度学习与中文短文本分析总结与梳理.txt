







文章大纲
1.绪论2.短文本2.1 短文本的研究范围2.2 短文本特点
3.中文分词3.1 基于字符串匹配和规则的分词方法3.2 基于统计的分词方法3.3 基于理解的分词方法
4. 基于深度学习的短文本分析[参考2]4.1 自动编码器4.2受限玻尔兹曼机4.3 卷积神经网络
5.为什么深度学习如此有效？[参考1]5.1主流深度学习模型对比
6.文本分类效果评价7.短文本相关工具应用7.1 Word2vector7.2 标签云7.3 倾向性分析7.4 成熟案例----微博舆情分析
8.主流分词工具对比分析8.1 总体介绍8.2 结巴和清华THULAC 介绍
9.最新短文本分析开源库分享9.1多语言词向量 Python 库
参考文献资源下载



1.绪论
过去几年，深度神经网络在模式识别中占绝对主流。它们在许多计算机视觉任务中完爆之前的顶尖算法。在语音识别上也有这个趋势了。而中文文本处理，以及中文自然语言处理上，似乎没有太厉害的成果？尤其是中文短文本处理的问题上，尚且没有太成功的应用于分布式条件下的深度处理模型？（大公司或许有，但没有开源）本文暂且梳理一下，尝试围绕深度学习和 短文本处理的方方面面就最简单的概念进行一次梳理，并且试图思考一个问题：
深度学习处理中文短文本的最终效果是什么？
我思考后的答案是：
答：是一种模型，可以无需任何语言学知识或手工特征设计，就可被用于中文分词、词性标注以及命名实体识别等多种中文自然语言处理任务，甚至直接改造为分布式大数据可以使用的框架。

2.短文本
姑且认为200字以内的都叫短文本
2.1 短文本的研究范围
- 搜索引擎的搜索结果
- 锚文本
- 互联网聊天信息
- 电子邮件主题
- 论坛评论信息
- 商品描述信息
- 图片描述
- 微博
- 手机短息
- 文档文献摘要

2.2 短文本特点
短文本具有特征稀疏性、奇异性、动态性、交错性等特点
①稀疏性。每条短文本形式信息的长度都比较短，都在 200 字以内，因此所包含的有效信息也就非常少，造成样本的特征非常稀疏，并且特征集的维数非常高，很难从中抽取到准确而关键的样本特征用于分类学习。
②实时性。在互联网上出现的短文本形式的信息，大部分都是实时更新的，刷新速度非常快，聊天信息、微博信息、评论信息等，并且文本数量非常庞大。
③不规则性。短文本形式的信息用语不规范，包含流行词汇较多，造成了噪声特征非常多，如“94”代表“就是”，“88”代表“再见”，“童鞋”代表“同学”，而且更新很快，如流行词“伤不起”、“有没有”、“坑爹” “屌丝”、等等。

3.中文分词
中文分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。众所周知，英文单词是用空格来进行分隔的，在中文的字与字之间、句子与句子之间、段落与段落之间也都能找到分界符。另外，在中英文中都存在短语划分这个问题，但是词与词之间却找不到形式上的分界符。词是最小的能够独立活动的有意义的语言成分，因而，中文在词的划分这个问题上是个非常巧杂而关键的问题。
现有的分词算法可分为3大类：
3.1 基于字符串匹配和规则的分词方法
基于字符串匹配的分词方法又称为机械分词方法，它是按照一定的策略将待分析的汉字与一个＂足够大的＂词典中词条进行匹配，若在词典中找到某个字符串，则匹配成功。按照扫描方向的不同，串匹配分词方法可分为正向匹配和逆向匹配；按照不同长度优先匹配的倩况，可分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相姐合，又可分为单纯分词方法和分词与标注相结合的一体化方法。
常用的基于字符串匹配的分词方法有：

A ）正向最大匹配法，按照文字的阅读顺序进行匹配； B ）逆向最大匹配法，按照文字的阅读顺序反向进行匹配； C ）最小切分法，使每一句中切出的词数量最少。由于汉语单字成词的特点，正向最小匹配和逆向最小匹配一般很少使用。 

正向最大匹配法
逆向匹配的切分精度略髙于正向匹配，遇到的歧义现象也较少。统计结果显示，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。但这种精度还远远不能满足实际的需要。
实际使用的分词系统，都是把机械分词作为一种初分手段，还需要通过利用各种其它的语言信息来进一步提高切分的准确率。上述方法虽然实现简单、速度快，但处理分词歧义能力较差，严重依赖于词表，不能识别新词语，即未登录词。为了解决分词歧义与未登录词的问题，９０年代初期出现了基于规则的分词系统，包括专家系统、短语结构文法等。基于规则的＂演泽推理＂方法，能较好的解决有规律的分词歧义和未登录词，具有一定的领域适应性、效率很髙。但中文语言现象非常复杂，存在很多无规律的分词歧义和未登录词。因此 一般采用其他算法如：动态规划等相结合提高准确率。
3.2 基于统计的分词方法
基于统计的分词方法只需对语料中的字信息进行统计，不需要切分词典，因而又称为无词典分词法或统计取词法。从形式上看，词是稳定的字的组合，在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映成词的可信度。因而可对语料中相邻共现的各个字的组合的频度进行统计，计算它们的相关度，计算两个汉字Ａ、Ｂ的相邻共现的概率。可对语料中相邻共现的各个字的组合的频率进行统计。
这种方法首先切分与词典能匹酷成功的所有可能的词，即找出所有候选词条，然后运用统计语言模型和决策算法得出最优的切分结果。
由于纯粹从统计的角度出发，因此在统计意义上某些经常出现在一起的字并不能构成完整的词语，例如＂上的＂、＂有的＂、＂这一＂等在文本中会大量的互邻同现,但它们却分属于不同的词；并且统计语言模型和决策算法在很大程度上决定了解决歧义的方法，需要大量的标注语料，并且分词速度也因搜索空间的増大而有所减慢。基于统计的分词方法所应用的主要的统计量或统计模型有：互信息、隐马尔可夫模型和最大熵模型等。这些统计模型主要是利用词与词之间的联合出现概率作为分词判断的信息。
3.3 基于理解的分词方法
这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果。其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括几个部分：分词子系统、句法语义子系统、总控部分。
在总控部分的协调下，分词子系统可获得有关词、句子等的句法和语义信息来对分词歧义进行判断，即它模拟了人对句子的理解过程。这种分词方法需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难Ｗ将各种语言信息组织成机器可直接读取的形式，因此目前基于机器学习理解的分词系统还处在实验阶段。
 基于隐马尔可夫模型的字标注中文分词方法。  基于层叠隐马尔可夫模型的汉语词法分析方法，该方法引入角色隐马尔可夫模型识别未登录词。然而，传统机器学习方法往往依赖于人工设计的特征，而一个特征是否有效需要多尝试与选择，因此人工设计一系列好的特征既费时又费力。  神经网络方法。  深度学习的方法，对中文语料进行中文分词和词性标注。这些方法仅接近与目前最好结果，并没有超越。 

4. 基于深度学习的短文本分析参考2
深度学习(Deep Learning)是一种表示学习方法，它通过对数据进行多层级的建模来获得关于数据特征的层次结构以及数据的分布式表示。由于深度学习可以避免繁琐的人工特征抽取，有效地利用无监督数据，并且具有优秀的泛化能力，因此成为了最近几年机器学习领域的一个热点。
深度学习的发展与应用使得图像处理、语音识别等多个具体的应用领域都取得了突破式的进展。深度学习非常适用于解决自然语言处理领域的一系列难题。
首先，由于语言本身的高维特性，传统的自然语言处理系统往往需要复杂的语言学知识以便手工构造出可供分类器使用的特征。而利用深度学习，则可以通过构造模型来自动学习用于解决自然语言处理领域的问题所需的特征。
其次，在自然语言处理领域，无标签数据可以被轻易地大量获得，然而有标签数据则相对稀少且昂贵，深度学习则刚好可以利用大量无标签数据来获取特征。再次，自然语言处理领域的许多问题往往相互之间具有非常强的关联性，例如对分词、词性标注和命名实体识别，传统的方法往往将这几个问题分开解决，忽略了它们之间的关系。使用深度学习则可以在特征抽取层面构造统一的模型以同时处理这些问题，并通过多任务学习的方法在模型中对其关联性进行建模，从而获得更好的性能。
因此，深度学习的核心思想是通过具有一定“深度”的模型从数据中逐层抽象出特征(即分布式表示)，并且在深度学习模型中越高的层级抽取出的特征具有越强的表达能力。据内部的特征信息，对于语言来说，这正是一个可以采纳的方式：从大量的文本中学习得到语义，并对语义进行特征表示，从而利用这些特征进行具体的计巧任务。
深度学习模型参考3

4.1 自动编码器
自动编码器（Auto—Encoder）是一种数据驱动的、非监督地学习数据特征的神经网络模型。其结构如所示。可Ｗ把它看成是一个输出节点数与输入节点数相等的多层神经网络。

4.2受限玻尔兹曼机
玻尔兹曼机（Boltzmann Machine）是一种引入了模拟退火思想的无向图模型，其根据无向图节点的状态和节点之间的互联权重定义整个系统的能量状态，并指定输入节点和输出节点为可见节点，在输入节点的二值化特征信息不变的情况下对其它节点进行退火，寻找最低的能量状态，即通过模拟退火的思想学习节点的互联权重作为模式的表示。
受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）是一种特殊的玻尔兹曼机，是深度置信网（Deep Belief Networks，DBN）的核心组件之一。它规定无向图节点分为可见层和隐含层两层，每层内部的节点之间不可互联，而处于不同层之间的节点为全连接

4.3 卷积神经网络
卷积神经网络主要包括两种神经层：卷积层（Ｃｏｎｖｏｌｕｔｉｏｎ）和池化层（又称下采样层，Ｐｏｏｌｉｎｇ）。这两层往往搭配出现。如图２－６所示一个典型的卷积神经网络模型一般包括数个卷积层与池化层的组合，在此之后的几层是神经网络的全连接层
卷积层是卷积神经网络的主要运算部分。卷积层的输入是一幅或多幅二维图像，这些图像被称为特征图像（ＦｅａｔｕｒｅＭａｐ）。每个输出特征图像对应一个卷积核，该卷积核对一幅或多幅输入图像进行卷积，卷积的结果取平均得到一幅输出特征图像
目前，深度学习在自然语言处理上取得的进展没有在语音图像上那么令人印象深刻。但相比于声音和图像，语言是唯一的非自然信号，是完全由人类大脑产生和处理的符号系统。因此，深度学习在自然语言处理领域中的应用研究仍存在诸多的挑战。

5.为什么深度学习如此有效？参考1
深度学习中的表征视角是非常有力的，比如单词嵌入深度学习可以将非常复杂的关系进行近似编码：

原文中这样说：
这看来是神经网络的一个非常强大的优点：它们能自动学习更好的数据表征的方法。反过来讲，能有效地表示数据对许多机器学习问题的成功都是必不可少的。单词嵌入仅仅是学习数据表示中一个引人注目的例子而已。
5.1主流深度学习模型对比
概述对比 
一个开源的深度学习测试框架参考9：对可扩展性（extensibility）、hardware utilization（硬件利用率）以及大家最关心的：速度（speed）上进行了比较
比较结果：

tensorflow比较中规中矩，我不认为其他几个库在后期能比他强多少，毕竟google出品，哈哈。现在不少企业都已经开始用tensorflow进行一些工程实践，大势所趋，同志们站好队啊。（欢迎同志们拍砖）

6.文本分类效果评价
很多时候，文本分析领域需要很多的评价准则，但是我看了不少论文，来回来去也就是基于分类的评价去做，这块还需要大牛给指点一二。
怎么能够客观的表示我们算法的准确行，分词，词性标注，等等
国际上广泛采用微平均和宏平均相结合的评价准则，并采用准确率P（Precision）和召回率R（Recall）以及F1值来衡量分类系统性能。
对第i个类别，其准确率和召回率分别定义如下：li表示分类的结果中被标记为第i类别且标记为正确的文本个数，mi表示结果中表示被标记为第i个类的文本个数，ni表示被分类的文本中实际属于第i个类别的样本个数。 
微平均和宏平均
微平均和宏平均是计算全局的查准率，查全率和F1测试值的两种方法。其中，微平均用mP、mR、mF1来表示：宏平均用MP、MR、MF1来表示，公式如下： 

7.短文本相关工具应用
7.1 Word2vector
word2vector由Google开发的一个用于训练语义向量的工具，其核心的技术是根据词频用Huffman编码使得所有词频相似的词隐藏层激活的巧内容基本一致，出现频率越高的词语，其隐藏层数目越少，然后采用一个三层神经网络对语义单元向量进行表示。具体可参考此说明文档参考10

快速入门:

7.2 标签云
回头打算，把自己的csdn博客爬一遍，写个python生成的标签云 哈哈，2018年12月这个系列已经开始了： 我给他起名叫做《简单中文NLP分析套路》---- 简单NLP分析套路（1）----语料库积累之3种简单爬虫应对大部分网站

7.3 倾向性分析
商品论坛评论（开门见山和卒章显志是汉语语篇的重要特点）

7.4 成熟案例----微博舆情分析



8.主流分词工具对比分析
8.1 总体介绍
主流分词概况：

分词准确度：

在所测试的四个数据集上，BosonNLP和哈工大语言云都取得了较高的分词准确率，尤其在新闻数据上。因为庖丁解牛是将所有可能成词的词语全部扫描出来（例如：“最不满意”分为：“最不 不满 满意”），与其他系统输出规范不同，因而不参与准确率统计。为了更直接的比较不同数据源的差别，我们从每个数据源的测试数据中抽取比较典型的示例进行更直观的对比。
大数据评测结果：jieba(c++)版独领风骚啊

8.2 结巴和清华THULAC 介绍
1.THULAC 清华新推荐的分词工具 参考8
THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：
能力强 利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。 准确率高 该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。 速度较快 分词速度同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。
2.Jieba 参考7
Jieba分词是中文分词领域使用非常广泛的自由软件，其采用MIT授权协议，支持繁体分词并且支持自定义词典。 Jieba分词支持三种分词模式：
1. 精确模式，试图将句子最精确地切开，适合文本分析；

2. 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；

3. 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

基本实现算法

基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG) 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法

分词速度

1.5 MB / Second in Full Mode 400 KB / Second in Default Mode 测试环境: Intel® Core™ i7-2600 CPU @ 3.4GHz；《围城》.txt


9.最新短文本分析开源库分享
9.1多语言词向量 Python 库
https://github.com/facebookresearch/MUSE
由 Facebook 开源的多语言词向量 Python 库，提供了基于 fastText 实现的多语言词向量和大规模高质量的双语词典，包括无监督和有监督两种。其中有监督方法使用双语词典或相同的字符串，无监督的方法不使用任何并行数据。
无监督方法具体可参考 Word Translation without Parallel Data 这篇论文。 ##9.2 FoolNLTK中文处理工具包
根据该项目所述，这个中文工具包的特点有如下几点：
可能不是最快的开源中文分词，但很可能是最准的开源中文分词基于 BiLSTM 模型训练而成包含分词，词性标注，实体识别, 都有比较高的准确率用户自定义词典
在中文信息处理中，分词（word segmentation）是一项基本技术，因为中文的词汇是彼此相连的，不像英文有一个天然的空格符可以分隔不同的单词。虽然把一串汉字划分成一个个词对于汉语使用者来说是很简单的事情，但对机器来说却很有挑战性，所以一直以来分词都是中文信息处理领域的重要的研究问题。
如该项目所述，作者使用了双向 LSTM 来构建整个模型，这也许是作者对分词性能非常有信心的原因。在中文分词上，基于神经网络的方法，往往使用「字向量 + 双向 LSTM + CRF」模型，利用神经网络来学习特征，将传统 CRF 中的人工特征工程量将到最低。

参考介绍： http://blog.csdn.net/Uwr44UOuQcNsUQb60zk2/article/details/78927752

参考文献
[参考1]: http://blog.jobbole.com/77709/
[参考2]: 李岩. 基于深度学习的短文本分析与计算方法研究[D]. 北京科技大学, 2016.
[参考3]: 吴轲. 基于深度学习的中文自然语言处理[D]. 东南大学, 2014.
[参考4]: http://www.52nlp.cn/
[参考5]: http://www.cnblogs.com/softidea/p/5981809.html
[参考6]: http://www.ltp-cloud.com/
[参考7]: https://github.com/fxsjy/jieba/
[参考8]:http://thulac.thunlp.org/
https://github.com/HIT-SCIR/ltp/

资源下载
短文本论文资源打包 http://download.csdn.net/detail/wangyaninglm/9793894

p.s. 第一版的这篇博客本来打算推荐这本《NLP汉语自然语言处理原理与实践》的，但是，我读了读发现，这书有点问题： 1.参照的核心代码是个java的，解释的不太到位 2.部分内容未经考证，盲目引用 书中说了一个九个字的姓，结果网上一查，有点怀疑真伪


3.价格偏贵，98元！几种概率图模型堆积公式并不是很清晰
我想了又想，认为，钱多的人买一本，钱少的，还是算了
http://www.threedweb.cn





