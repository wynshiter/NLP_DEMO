







文章大纲

前言
预训练模型简介


语言表示学习
神经上下文编码器
为何需要预训练模型
发展历史
 
主流预训练模型
预训练模型与分类
将 PTMs 应用至下游任务


微调策略
 
未来研究方向
参考文献




前言
随着深度学习的发展，各种神经网络被广泛用于解决自然语言处理(NLP)任务，如卷积神经网络(convolutional neural networks, CNNs)、递归神经网络(neural networks, RNNs)、基于图的神经网络(graphbased neural network, GNNs)和注意力机制等。这些神经模型的优点之一是能够缓解特征工程问题。非神经NLP方法通常严重依赖于离散的手工特征，而神经方法通常使用低维和稠密的向量(又称分布式表示)隐式地表示语言的语法或语义特征。这些表示是在特定的NLP任务中学习的。因此，神经方法使人们可以很容易地开发各种NLP系统。
尽管神经模型在NLP任务中取得了成功，但与计算机视觉(CV)领域相比，性能改进可能不那么显著。主要原因是，当前用于大多数监督的NLP任务的数据集相当小(机器




