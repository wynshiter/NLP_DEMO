





介绍
Pandas 是非常著名的开源数据处理库，我们可以通过它完成对数据集进行快速读取、转换、过滤、分析等一系列操作。除此之外，Pandas 拥有强大的缺失数据处理与数据透视功能，可谓是数据预处理中的必备利器。
知识点
数据类型数据读取数据选择数据删减数据填充
Pandas 是非常著名的开源数据处理库，其基于 NumPy 开发，该工具是 Scipy 生态中为了解决数据分析任务而设计。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的函数和方法。 特有的数据结构是 Pandas 的优势和核心。简单来讲，我们可以将任意格式的数据转换为 Pandas 的数据类型，并使用 Pandas 提供的一系列方法进行转换、操作，最终得到我们期望的结果。 所以，我们首先需要了解并熟悉 Pandas 支持的数据类型。
数据类型
Pandas 的数据类型主要有以下几种，它们分别是：Series（一维数组），DataFrame（二维数组），Panel（三维数组），Panel4D（四维数组），PanelND（更多维数组）。其中 Series 和 DataFrame 应用的最为广泛，几乎占据了使用频率 90% 以上。
Series
 Series 是 Pandas 中最基本的一维数组形式。其可以储存整数、浮点数、字符串等类型的数据。Series 基本结构如下：
pandas.Series(data=None, index=None)

其中，data 可以是字典，或者NumPy 里的 ndarray 对象等。index 是数据索引，索引是 Pandas 数据结构中的一大特性，它主要的功能是帮助我们更快速地定位数据。
下面，我们基于 Python 字典新建一个示例 Series。
%matplotlib inline
import pandas as pd

s = pd.Series({'a': 10, 'b': 20, 'c': 30})
s

a    10
b    20
c    30
dtype: int64

如上所示，该 Series 的数据值是 10, 20, 30，索引为 a, b, c，数据值的类型默认识别为 int64。你可以通过 type 来确认 s 的类型。
type(s)

pandas.core.series.Series

由于 Pandas 基于 NumPy 开发。那么 NumPy 的数据类型 ndarray 多维数组自然就可以转换为 Pandas 中的数据。而 Series 则可以基于 NumPy 中的一维数据转换。
import numpy as np

s = pd.Series(np.random.randn(5))
s

0    0.610231
1   -0.365473
2   -1.101726
3   -1.076163
4    0.037320
dtype: float64

如上所示，我们给出了 NumPy 生成的一维随机数组，最终得到的 Series 索引默认从 0 开始，而数值类型为 float64。
DataFrame
DataFrame 是 Pandas 中最为常见、最重要且使用频率最高的数据结构。DataFrame 和平常的电子表格或 SQL 表结构相似。你可以把 DataFrame 看成是 Series 的扩展类型，它仿佛是由多个 Series 拼合而成。它和 Series 的直观区别在于，数据不但具有行索引，且具有列索引。

DataFrame 基本结构如下：
pandas.DataFrame(data=None, index=None, columns=None)

区别于 Series，其增加了 columns 列索引。DataFrame 可以由以下多个类型的数据构建：
一维数组、列表、字典或者 Series 字典。二维或者结构化的 numpy.ndarray。一个 Series 或者另一个 DataFrame。
例如，我们首先使用一个由 Series 组成的字典来构建 DataFrame。
df = pd.DataFrame({'one': pd.Series([1, 2, 3]),
                   'two': pd.Series([4, 5, 6])})
df



onetwo014125236
当不指定索引时，DataFrame 的索引同样是从 0 开始。我们也可以直接通过一个列表构成的字典来生成 DataFrame。
df = pd.DataFrame({'one': [1, 2, 3],
                   'two': [4, 5, 6]})
df



onetwo014125236
或者反过来，由带字典的列表生成 DataFrame。
df = pd.DataFrame([{'one': 1, 'two': 4},
                   {'one': 2, 'two': 5},
                   {'one': 3, 'two': 6}])
df



onetwo014125236
NumPy 的多维数组非常常用，同样可以基于二维数值来构建一个 DataFrame。
pd.DataFrame(np.random.randint(5, size=(2, 4)))



01230201212410
至此，你应该已经清楚了 Pandas 常用的 Series 和 DataFrame 数据类型。Series 实际上可以被初略看出是只有 1 列数据的 DataFrame。当然，这个说法不严谨，二者的核心区别仍然是 Series 没有列索引。你可以观察如下所示由 NumPy 一维随机数组生成的 Series 和 DataFrame。
pd.Series(np.random.randint(5, size=(5,)))

0    3
1    2
2    4
3    2
4    3
dtype: int32

pd.DataFrame(np.random.randint(5, size=(5,)))



00314233440
关于 Pandas 中的 Panel 等数据类型我们就不再介绍。首先是这些数据类型用的很少，其次就算你用到了，也可以通过从 DataFrame 等学到的技巧进行迁移应用，万变不离其宗。
数据读取
我们想要使用 Pandas 来分析数据，那么首先需要读取数据。大多数情况下，数据都来源于外部的数据文件或者数据库。Pandas 提供了一系列的方法来读取外部数据，非常全面。下面，我们以最常用的 CSV 数据文件为例进行介绍。
读取数据 CSV 文件的方法是 pandas.read_csv()，你可以直接传入一个相对路径，或者是网络 URL。
df = pd.read_csv("test.csv")
df



表头1表头2表头3表头4表头501.02晴天1112.03下雨2120.1-1下雪3134.05无4140.00weather5159.08season6169.08NaN61
由于 CSV 存储时是一个二维的表格，那么 Pandas 会自动将其读取为 DataFrame 类型。 现在你应该就明白了，DataFrame 是 Pandas 构成的核心。一切的数据，无论是外部读取还是自行生成，我们都需要先将其转换为 Pandas 的 DataFrame 或者 Series 数据类型。实际上，大多数情况下，这一切都是设计好的，无需执行额外的转换工作。
pd.read_ 前缀开始的方法还可以读取各式各样的数据文件，且支持连接数据库。这里，我们不再依次赘述，你可以阅读  官方文档相应章节 熟悉这些方法以及搞清楚这些方法包含的参数。
你可能又一个疑问：为什么要将数据转换为 Series 或者 DataFrame 结构？
实际上，我现在就可以先回答这个问题。因为 Pandas 针对数据操作的全部方法都是基于 Pandas 支持的数据结构设计的。也就是说，只有 Series 或者 DataFrame 才能使用 Pandas 提供的方法和函数进行处理。所以，学习真正数据处理方法之前，我们需要将数据转换生成为 Series 或 DataFrame 类型。
基本操作
通过上面的内容，我们已经知道一个 DataFrame 结构大致由 3 部分组成，它们分别是列名称、索引和数据。


 来源

接下来，我们就学习针对 DataFrame 的基本操作。本次课程中，我们不会刻意强调 Series，因为你在 DataFrame 上学习的大多数方法和技巧都适用于对 Series 进行处理，二者同根同源。
上面，我们已经读取了一个外部数据，这是洛杉矶的人口普查数据。有些时候，我们读取的文件很大。如果全部输出预览这些文件，既不美观，又很耗时。还好，Pandas 提供了 head() 和 tail() 方法，它可以帮助我们只预览一小块数据。
df.head()  # 默认显示前 5 条



表头1表头2表头3表头4表头501.02晴天1112.03下雨2120.1-1下雪3134.05无4140.00weather51
df.tail(7)  # 指定显示后 7 条



表头1表头2表头3表头4表头501.02晴天1112.03下雨2120.1-1下雪3134.05无4140.00weather5159.08season6169.08NaN61
Pandas 还提供了统计和描述性方法，方便你从宏观的角度去了解数据集。describe() 相当于对数据集进行概览，会输出该数据集每一列数据的计数、最大值、最小值等。
df.describe()



表头1表头2表头4表头5count7.0000007.0000007.0000007.0mean3.5857143.5714293.8571431.0std3.9371853.5989421.9518000.0min0.000000-1.0000001.0000001.025%0.5500001.0000002.5000001.050%2.0000003.0000004.0000001.075%6.5000006.5000005.5000001.0max9.0000008.0000006.0000001.0
Pandas 基于 NumPy 开发，所以任何时候你都可以通过 .values 将 DataFrame 转换为 NumPy 数组。
df.values

array([[1.0, 2, '晴天', 1, 1],
       [2.0, 3, '下雨', 2, 1],
       [0.1, -1, '下雪', 3, 1],
       [4.0, 5, '无', 4, 1],
       [0.0, 0, 'weather', 5, 1],
       [9.0, 8, 'season', 6, 1],
       [9.0, 8, nan, 6, 1]], dtype=object)

这也就说明了，你可以同时使用 Pandas 和 NumPy 提供的 API 对同一数据进行操作，并在二者之间进行随意转换。这就是一个非常灵活的工具生态圈。
除了 .values，DataFrame 支持的常见属性可以通过  官方文档相应章节 查看。其中常用的有：
df.index  # 查看索引

RangeIndex(start=0, stop=7, step=1)

df.columns  # 查看列名

Index(['表头1', '表头2', '表头3', '表头4', '表头5'], dtype='object')

df.shape  # 查看形状

(7, 5)

数据选择
在数据预处理过程中，我们往往会对数据集进行切分，只将需要的某些行、列，或者数据块保留下来，输出到下一个流程中去。这也就是所谓的数据选择，或者数据索引。
由于 Pandas 的数据结构中存在索引、标签，所以我们可以通过多轴索引完成对数据的选择。
基于索引数字选择
当我们新建一个 DataFrame 之后，如果未自己指定行索引或者列对应的标签，那么 Pandas 会默认从 0 开始以数字的形式作为行索引，并以数据集的第一行作为列对应的标签。其实，这里的「列」也有数字索引，默认也是从 0 开始，只是未显示出来。
所以，我们首先可以基于数字索引对数据集进行选择。这里用到的 Pandas 中的 .iloc 方法。该方法可以接受的类型有：
整数。例如：5整数构成的列表或数组。例如：[1, 2, 3]布尔数组。可返回索引值的函数或参数。
下面，我们使用上方的示例数据进行演示。 首先，我们可以选择前 3 行数据。这和 Python 或者 NumPy 里面的切片很相似。
df.iloc[:3]



表头1表头2表头3表头4表头501.02晴天1112.03下雨2120.1-1下雪31
我们还可以选择特定的一行。
df.iloc[1]

表头1     2
表头2     3
表头3    下雨
表头4     2
表头5     1
Name: 1, dtype: object

那么选择多行，是不是 df.iloc[1, 3, 5] 这样呢？
答案是错误的。df.iloc[] 的 [[行]，[列]] 里面可以同时接受行和列的位置，如果你直接键入 df.iloc[1, 3, 5] 就会报错。
所以，很简单。如果你想要选择 1，3，5 行，可以这样做。
df.iloc[[1, 3, 5]]



表头1表头2表头3表头4表头512.03下雨2134.05无4159.08season61
选择行学会以后，选择列就应该能想到怎么办了。例如，我们要选择第 2-4 列。
df.iloc[:, 1:4]



表头2表头3表头402晴天113下雨22-1下雪335无440weather558season668NaN6
这里选择 2-4 列，输入的却是 1:4。这和 Python 或者 NumPy 里面的切片操作非常相似。既然我们能定位行和列，那么只需要组合起来，我们就可以选择数据集中的任何数据了。
基于标签名称选择
除了根据数字索引选择，还可以直接根据标签对应的名称选择。这里用到的方法和上面的 iloc 很相似，少了个 i 为 df.loc[]。
df.loc[] 可以接受的类型有：
单个标签。例如：2 或 'a'，这里的 2 指的是标签而不是索引位置。列表或数组包含的标签。例如：['A', 'B', 'C']。切片对象。例如：'A':'E'，注意这里和上面切片的不同之处，首尾都包含在内。布尔数组。可返回标签的函数或参数。
下面，我们来演示 df.loc[] 的用法。先选择前 3 行：
df.loc[0:2]



表头1表头2表头3表头4表头501.02晴天1112.03下雨2120.1-1下雪31
再选择 1，3，5 行：
df.loc[[0, 2, 4]]



表头1表头2表头3表头4表头501.02晴天1120.1-1下雪3140.00weather51
然后，选择 2-4 列：
df.loc[:, '表头2':'表头4']



表头2表头3表头402晴天113下雨22-1下雪335无440weather558season668NaN6
最后，选择 1，3 行和 表头3 后面的列：
df.loc[[0, 2], '表头3':]



表头3表头4表头50晴天112下雪31
数据删减
虽然我们可以通过数据选择方法从一个完整的数据集中拿到我们需要的数据，但有的时候直接删除不需要的数据更加简单直接。Pandas 中，以 .drop 开头的方法都与数据删减有关。
DataFrame.drop 可以直接去掉数据集中指定的列和行。一般在使用时，我们指定 labels 标签参数，然后再通过 axis 指定按列或按行删除即可。当然，你也可以通过索引参数删除数据，具体查看官方文档。
df.drop(labels=['表头2','表头4'], axis=1)



表头1表头3表头501.0晴天112.0下雨120.1下雪134.0无140.0weather159.0season169.0NaN1
DataFrame.drop_duplicates 则通常用于数据去重，即剔除数据集中的重复值。使用方法非常简单，指定去除重复值规则，以及 axis 按列还是按行去除即可。
df.drop_duplicates(['表头5'])



表头1表头2表头3表头4表头501.02晴天11
除此之外，另一个用于数据删减的方法 DataFrame.dropna 也十分常用，其主要的用途是删除缺少值，即数据集中空缺的数据列或行。
df.dropna()



表头1表头2表头3表头4表头501.02晴天1112.03下雨2120.1-1下雪3134.05无4140.00weather5159.08season61
对于提到的这 3 个常用的数据删减方法，大家一定要通过给出的链接去阅读官方文档。这些常用方法没有太多需要注意的地方，通过文档了解其用法即可，所以我们也不会化简为繁地进行介绍。
数据填充
在真实的生产环境中，我们需要处理的数据文件往往没有想象中的那么美好。其中，很大几率会遇到的情况就是缺失值。缺失值主要是指数据丢失的现象，也就是数据集中的某一块数据不存在。除此之外、存在但明显不正确的数据也被归为缺失值一类。例如，在一个时间序列数据集中，某一段数据突然发生了时间流错乱，那么这一小块数据就是毫无意义的，可以被归为缺失值。
检测缺失值
Pandas 为了更方便地检测缺失值，将不同类型数据的缺失均采用 NaN 标记。这里的 NaN 代表 Not a Number，它仅仅是作为一个标记。例外是，在时间序列里，时间戳的丢失采用 NaT 标记。
Pandas 中用于检测缺失值主要用到两个方法，分别是：isna() 和 notna()，故名思意就是「是缺失值」和「不是缺失值」。默认会返回布尔值用于判断。
接下来，我们人为生成一组包含缺失值的示例数据。
df = pd.DataFrame(np.random.rand(9, 5), columns=list('ABCDE'))
# 插入 T 列，并打上时间戳
df.insert(value=pd.Timestamp('2017-10-1'), loc=0, column='Time')
# 将 1, 3, 5 列的 1，3，5，7 行置为缺失值
df.iloc[[1, 3, 5, 7], [0, 2, 4]] = np.nan
# 将 2, 4, 6 列的 2，4，6，8 行置为缺失值
df.iloc[[2, 4, 6, 8], [1, 3, 5]] = np.nan
df



TimeABCDE02017-10-010.7832640.2713180.2293900.4287000.7801101NaT0.435561NaN0.000583NaN0.68769022017-10-01NaN0.186035NaN0.721837NaN3NaT0.465740NaN0.329557NaN0.72813842017-10-01NaN0.782575NaN0.042452NaN5NaT0.333087NaN0.624744NaN0.00567362017-10-01NaN0.774094NaN0.959860NaN7NaT0.139802NaN0.570984NaN0.33573682017-10-01NaN0.990180NaN0.569772NaN
然后，通过 isna() 或 notna() 中的一个即可确定数据集中的缺失值。
df.isna()



TimeABCDE0FalseFalseFalseFalseFalseFalse1TrueFalseTrueFalseTrueFalse2FalseTrueFalseTrueFalseTrue3TrueFalseTrueFalseTrueFalse4FalseTrueFalseTrueFalseTrue5TrueFalseTrueFalseTrueFalse6FalseTrueFalseTrueFalseTrue7TrueFalseTrueFalseTrueFalse8FalseTrueFalseTrueFalseTrue
上面已经对缺省值的产生、检测进行了介绍。实际上，面对缺失值一般就是填充和剔除两项操作。填充和清除都是两个极端。如果你感觉有必要保留缺失值所在的列或行，那么就需要对缺失值进行填充。如果没有必要保留，就可以选择清除缺失值。
其中，缺失值剔除的方法 dropna() 已经在上面介绍过了。下面来看一看填充缺失值 fillna() 方法。
首先，我们可以用相同的标量值替换 NaN，比如用 0。
df.fillna(0)



TimeABCDE02017-10-01 00:00:000.7832640.2713180.2293900.4287000.780110100.4355610.0000000.0005830.0000000.68769022017-10-01 00:00:000.0000000.1860350.0000000.7218370.000000300.4657400.0000000.3295570.0000000.72813842017-10-01 00:00:000.0000000.7825750.0000000.0424520.000000500.3330870.0000000.6247440.0000000.00567362017-10-01 00:00:000.0000000.7740940.0000000.9598600.000000700.1398020.0000000.5709840.0000000.33573682017-10-01 00:00:000.0000000.9901800.0000000.5697720.000000
除了直接填充值，我们还可以通过参数，将缺失值前面或者后面的值填充给相应的缺失值。例如使用缺失值前面的值进行填充：
df.fillna(method='pad')



TimeABCDE02017-10-010.7832640.2713180.2293900.4287000.78011012017-10-010.4355610.2713180.0005830.4287000.68769022017-10-010.4355610.1860350.0005830.7218370.68769032017-10-010.4657400.1860350.3295570.7218370.72813842017-10-010.4657400.7825750.3295570.0424520.72813852017-10-010.3330870.7825750.6247440.0424520.00567362017-10-010.3330870.7740940.6247440.9598600.00567372017-10-010.1398020.7740940.5709840.9598600.33573682017-10-010.1398020.9901800.5709840.5697720.335736
或者是后面的值：
df.fillna(method='bfill')



TimeABCDE02017-10-010.7832640.2713180.2293900.4287000.78011012017-10-010.4355610.1860350.0005830.7218370.68769022017-10-010.4657400.1860350.3295570.7218370.72813832017-10-010.4657400.7825750.3295570.0424520.72813842017-10-010.3330870.7825750.6247440.0424520.00567352017-10-010.3330870.7740940.6247440.9598600.00567362017-10-010.1398020.7740940.5709840.9598600.33573672017-10-010.1398020.9901800.5709840.5697720.33573682017-10-01NaN0.990180NaN0.569772NaN
最后一行由于没有对于的后序值，自然继续存在缺失值。 上面的例子中，我们的缺失值是间隔存在的。那么，如果存在连续的缺失值是怎样的情况呢？试一试。首先，我们将数据集的第 2，4 ，6 列的第 3，5 行也置为缺失值。
df.iloc[[3, 5], [1, 3, 5]] = np.nan

然后来正向填充：
df.fillna(method='pad')



TimeABCDE02017-10-010.7832640.2713180.2293900.4287000.78011012017-10-010.4355610.2713180.0005830.4287000.68769022017-10-010.4355610.1860350.0005830.7218370.68769032017-10-010.4355610.1860350.0005830.7218370.68769042017-10-010.4355610.7825750.0005830.0424520.68769052017-10-010.4355610.7825750.0005830.0424520.68769062017-10-010.4355610.7740940.0005830.9598600.68769072017-10-010.1398020.7740940.5709840.9598600.33573682017-10-010.1398020.9901800.5709840.5697720.335736
可以看到，连续缺失值也是按照前序数值进行填充的，并且完全填充。这里，我们可以通过 limit= 参数设置连续填充的限制数量。
df.fillna(method='pad', limit=1)  # 最多填充一项

除了上面的填充方式，还可以通过 Pandas 自带的求平均值方法等来填充特定列或行。举个例子：
df.fillna(df.mean()['C':'E'])

D:\ProgramData\Anaconda3\envs\NLP_DEMO\lib\site-packages\ipykernel_launcher.py:1: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.
  """Entry point for launching an IPython kernel.



TimeABCDE02017-10-010.7832640.2713180.2293900.4287000.7801101NaT0.435561NaN0.0005830.5445240.68769022017-10-01NaN0.1860350.2669860.7218370.6011793NaTNaNNaN0.2669860.5445240.60117942017-10-01NaN0.7825750.2669860.0424520.6011795NaTNaNNaN0.2669860.5445240.60117962017-10-01NaN0.7740940.2669860.9598600.6011797NaT0.139802NaN0.5709840.5445240.33573682017-10-01NaN0.9901800.2669860.5697720.601179
对 C 列到 E 列用平均值填充。
插值填充
插值是数值分析中一种方法。简而言之，就是借助于一个函数（线性或非线性），再根据已知数据去求解未知数据的值。插值在数据领域非常常见，它的好处在于，可以尽量去还原数据本身的样子。
我们可以通过 interpolate() 方法完成线性插值。当然，其他一些插值算法可以阅读官方文档了解。
# 生成一个 DataFrame
df = pd.DataFrame({'A': [1.1, 2.2, np.nan, 4.5, 5.7, 6.9],
                   'B': [.21, np.nan, np.nan, 3.1, 11.7, 13.2]})
df



AB01.10.2112.2NaN2NaNNaN34.53.1045.711.7056.913.20
对于上面存在的缺失值，如果通过前后值，或者平均值来填充是不太能反映出趋势的。这时候，插值最好使。我们用默认的线性插值试一试。
df_interpolate = df.interpolate()
df_interpolate



AB01.100.21000012.201.17333323.352.13666734.503.10000045.7011.70000056.9013.200000
下图展示了插值后的数据，明显看出插值结果符合数据的变化趋势。如果按照前后数据顺序填充，则无法做到这一点。
对于 interpolate() 支持的插值算法，也就是 method=。下面给出几条选择的建议：
如果你的数据增长速率越来越快，可以选择 method='quadratic'二次插值。如果数据集呈现出累计分布的样子，推荐选择 method='pchip'。如果需要填补缺省值，以平滑绘图为目标，推荐选择 method='akima'。
当然，最后提到的 method='akima'，需要你的环境中安装了 Scipy 库。除此之外，method='barycentric' 和 method='pchip' 同样也需要 Scipy 才能使用。
数据可视化
NumPy，Pandas，Matplotlib 构成了一个完善的数据分析生态圈，所以 3 个工具的兼容性也非常好，甚至共享了大量的接口。当我们的数据是以 DataFrame 格式呈现时，可以直接使用 Pandas 提供的 DataFrame.plot 方法调用 Matplotlib 接口绘制常见的图形。
例如，我们使用上面插值后的数据 df_interpolate 绘制线形图。
df_interpolate.plot()

<AxesSubplot:>


其他样式的图形也很简单，指定 kind= 参数即可。
df_interpolate.plot(kind='bar')

<AxesSubplot:>


更多的图形样式和参数，阅读官方文档中的详细说明。Pandas 绘图虽然不可能做到 Matplotlib 的灵活性，但是其简单易用，适合于数据的快速呈现和预览。
其他用法
由于 Pandas 包含的内容实在太多，除了阅读完整的官方文档，很难做到通过一个实验或者一个课程进行全面了解。当然，本课程的目的是带大家熟悉 Pandas 的常用基础方法，至少你大致清楚了 Pandas 是什么，能干什么。
除了上面提到的一些方法和技巧，实际上 Pandas 常用的还有：
 数据计算，例如：DataFrame.add 等。 数据聚合，例如：DataFrame.groupby 等。 统计分析，例如：DataFrame.abs 等。 时间序列，例如：DataFrame.shift 等。

参考文献
https://www.datacamp.com/community/blog/python-pandas-cheat-sheet https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python https://github.com/pandas-dev/pandas/tree/master/doc/cheatsheet
https://www.lanqiao.cn/courses/906
参考本人文章
大数据ETL实践探索（5）---- 大数据ETL利器之 pandas浅谈pandas，pyspark 的大数据ETL实践经验大数据ETL实践探索（9）---- 使用pandas sqlalchemy 以及多进程进行百万级 postgresSQL 数据入库python Pandas Profiling 一行代码EDA 探索性数据分析




