







文章大纲

简介


Gpt-2简述
与 BERT 的区别
GPT-2 内部机制


1. 输入编码
2.两个权值矩阵
3. 堆栈
4. 回顾自注意力机制
5. 模型输出
 
 
三行代码生成文章
没有好机器不用做的gpt-2 文本生成 实验
狗屁不通文章生成器
参考文档




简介
文本生成目前主要试用的是GPT-2 模型
基本上只要了解 Transformer 架构，你馬上就懂 GPT-2 了。因為该语言模型的本质上就是 Transformer 里的 Decoder： 




