




写在前面，此文的目的只是单纯地跟大家分享自己对感知机学习的一些感悟。并不是科普文，对感知机没有认识的朋友可能并不能从此文得到清晰的概念，如果要学习感知机还是建议看经典的教材。
        ==========正文==========
想写这样的文章很久了。至于动机，说来话长。先允许我矫情的自我介绍一下。
我的成长环境相对来说比较封闭，传统甚至是严苛。导致我性格当中有很多紧张的成分，同时不善于交往。直到碰到为数不多的朋友前，我的人际关系包括个人感情，不出所料的“一塌糊涂”。于是某一时刻，我想结束这种自卑与尴尬。
我的选择也许你不能理解——探索“人工智能”。
因为我不能左右逢源，因为不能做大众眼中的“聪明人”，所以我想用这种方式去接近“智能”这个概念。以此探究什么是“聪明”
因为我总是遭遇“尴尬”，所以我想用探究“智能”的方式来理解“成长”或者说“学习”的本质。
所以我所写的文章，我所做的思考，总在把“智能学习”投射到“人生思考”上。我不知道有没有造成“牵强”的感觉，实际上我只是在尽量表达我的理解。
研究生我选择模式识别和图像处理，那一刻我觉得自己充满了力量，不在寄托他人或他物来寻求内心的平衡。
接下来的学习并没有令我失望。实际上当第一次接触到“感知机”——最简单的机器学习模型时，我有种豁然开朗的感觉。

感知机，模仿神经元，是神经网络中的基本单元。
它有多个输入，一个输出。
输入后，每个输入Xi乘以相应权重Wi相加，其总和再通过与阈值B相减，得到一个大于等于0或小于0的值，若大于0，输出1，否则输出-1。
现在如果感知机被训练“成熟”，能正确输出“+1”、“-1”，判断“是”、“非”。即，将所有样本点正确分类。那么想象它的权重向量W和偏移B，构成了一个超平面。这个超平面把这个世界一分为二，若一边“是”的样本，一边“非”的样本，它本身，就是是非的准则。

在面对两类样本，要完成“是”与“非”的判断时
感知器傻傻的伸出一条线Y=WX+B(此时W,B有随机性)，试图探索。然而除非你是“神派来的宠儿”，否则面对新的事物，无法“一蹴而就”。
事实是这样，感知器毫不意外地得到了错误的分类。即：我们在对的世界里抽取一个样本x给感知器就判断，试图得到+1。但是由于错误的判断，感知器返回了-1。
也就是按照刚才那条直线y=wx+b去探索，得到y=-1。
此时的感知器不会像人一样陷入自责、畏惧，而是自然而然的调整方向。
怎么调整？既然不对，我就回头。
怎么回头？
首先调整“是非”判断整体倾向，再向本次经验“对”的方向靠拢。

如果用绘制图像来阐述，那就是，如果“是”判断成了“非”，超平面首先朝“是”的反方向平移，相当于降低阈值。这样造成我们判断的结果整体倾向于“是”。
接着，我们假设此时空间样本点为X0,那么其正确结果应为“是”，即“+1”,我们把空间点(X0,1)构成的向量称作V，是我们期待的正确方向；现在我们的分类超平面Y=WX+B，试图朝着方向V旋转，以做调整。
这种调整如何得来，为什么它是正确的？
因为如果我们把X0样本点进行了错误归类，产生错误结果-1。那么这次错误的损失可以量化为-(WX0+B)。当X0已知，我们想针对(W,B)寻求一种变化，这种变化的结果使以上损失接近最小值，那就是沿着的梯度方向走，也就是朝变化最快的方向调整。
-WX0-B分别对W,B求偏导，得出最佳调整方向(-X0,-1)
W=W-X0
B=B-1
如果直观一点来讲，如果你认为一个人秃顶程度象征一个人的猥琐程度，那么猥琐到一定程度你把他归类为一个“坏人”。

直到有一天你遇到一个秃顶的程序员，你发现他很善良。这个事实对你来说，就是样本X0。
于是你开始调整自己关于“猥琐”这个观念的分类线Y=WX+B。
首先你调整B，把分类为“坏人”的阈值提高，也就是说，不再轻易把一个人定义为“坏人”。
B=B-1。
第二步你调整W，因为之前你认为秃顶是猥琐的，所以错判了一个安分的人，所以你现在朝着相反的方向改变这个观念，当然不是从此判定“秃顶”是不猥琐的，而是，以前认为“秃顶”是“猥琐”的观念不再那么根深蒂固，换句话来说，你并未因为照顾这个样本走到另一个极端，而是背离这个带给你一次错误的观念，回头那么一点点。
W=W-X0。
当然人是立体的，你判断一个人，一个事物也是立体的，所以你观念中的维数，不止上述一维。多维情况只是一维的简单扩展。
用简单一点的话概括，如果产生错误的判断。先调整阈值，再在每一维参与判断的权重中，向相反的方向回头调整
不用畏惧笨手笨脚，学习的本质就是实践和探索，在“样本”之间调整，而这个调整，也没想象的那么复杂，遇到“不对”的，回头就是。无数次碰壁与回头，总能找到最佳的是非状态。
然而，回头，是走向另一种极端吗？不是，是在前面的经验的调整而已
那么，为什么我们自信可以用这种方法调整到最佳分类状态。因为感知机的收敛性早已得到了充分的证明。
感知机算法在训练数据集上的误分类次数k满足不等式

这个收敛性证明是建立在两个假设上(证明略)
(1)假设样本是线性可分的，即存在一个最优超平面将两类样本分开。样本点到最优分类面的最短距离为
(2)假设样本点的长度是有上界的。上界为

误分类的次数k是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。
也就是有限次尝试，我们可以达到最佳状态。
我们经常畏惧失败，焦虑，强迫自己学会应对复杂的问题，多次失败后往往把我们打击的体无完肤。但是却忽略了简单的本质，感知机怎么看起来像个傻傻的但不屈不挠的自己呢。这个世界给你带来的痛苦，只不过是你所经历的“样本”罢了，take it easy，你只要学会调整，回头，不要走到极端，你总会找到那个最佳的状态。

当然，也可能不止这么简单。
比如这个世界很复杂，那么就是说判断的维数很高，那么我们就提高样本的数量使“分类器”正确，就是我们遇到的事情越复杂，我们经历的尝试就应该越多，才能不落入“片面”或“不成熟”的看法。
比如这个世界是非难分，那就是碰到了“线性不可分”的样本，那么我们就增加认识的深度，也就是给样本增加维度去看(支持向量机)，它也许，就变得清晰可分了。
总而言之，我对“智能”“学习”的粗浅感悟就是，这个世界总是客观的，收起对它过度的情绪，置身物外，拿起理智，尝试去认识它。




